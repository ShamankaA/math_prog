{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShamankaA/math_prog/blob/main/6.%20%D0%90%D0%B2%D1%82%D0%BE%D1%8D%D0%BD%D0%BA%D0%BE%D0%B4%D0%B5%D1%80%D1%8B/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%966_%D0%9F%D1%80%D0%BE%D0%B5%D0%BA%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%B0%D0%B2%D1%82%D0%BE%D1%8D%D0%BD%D0%BA%D0%BE%D0%B4%D0%B5%D1%80%D0%BE%D0%B2_%D0%B4%D0%BB%D1%8F_%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D1%8F_%D0%BF%D1%80%D0%B8%D0%BA%D0%BB%D0%B0%D0%B4%D0%BD%D1%8B%D1%85_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Практическая работа №6. Проектирование автоэнкодеров для решения прикладных задач**\n",
        "\n",
        "[**Ссылка на код с пары**](https://colab.research.google.com/drive/1KfCEQly5k0PUdCR1w-my2-QmYfFexdvK?usp=sharing)"
      ],
      "metadata": {
        "id": "YAh9H5lgA0kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### - **N.B.: Во всех заданиях, графический интерфейс для взаимодействия с обученной моделью, реализуется с помощью Gradio!**\n",
        "\n",
        "### - **Для каждого задания в графическом интерфейсе должно быть встроено минимум 3 примера (sample)**"
      ],
      "metadata": {
        "id": "aQv_0pqQFgIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание №1. Реализуйте автоэнкодер для колоризации чёрно-белых изображений\n",
        "\n",
        "* **Чем больше объектов разных классов будет в исходном датасете, тем универсальнее будет работа обученной Вами модели**\n",
        "\n",
        "> Например, если в Вашем датасете только кошки, то все объекты кроме кошек будут колоризоваться некорректно. Соответственно следует очень тщательно подойти к выбору датасета.\n",
        "\n",
        "  * Для решения данной задачи может подойти датасет [CIFAR-100](https://www.kaggle.com/datasets/fedesoriano/cifar100), но его минус заключается в том, что разрешение изображений довольно низкое\n",
        "\n",
        "* Хорошим вариантом будет использование датасета [ImageNet](https://paperswithcode.com/dataset/imagenet), но ввиду его объёмности, процесс обучения займет большое количество времени, поэтому Вы можете использовать одну из его [сокращенных версий](https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000)\n",
        "\n",
        " **Конечный выбор датасета осуществляется по Вашему желанию, учитывая рекомендации приведенные выше.**\n"
      ],
      "metadata": {
        "id": "i1OE2rQxqMps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт библиотек\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import numpy as np\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "lAfLwihcpJpt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка датасета CIFAR-100\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.cifar100.load_data()"
      ],
      "metadata": {
        "id": "JvQO1xdwpNBj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Нормализация и преобразование в ч/б\n",
        "x_train_gray = tf.image.rgb_to_grayscale(x_train / 255.0)\n",
        "x_test_gray = tf.image.rgb_to_grayscale(x_test / 255.0)\n",
        "x_train_color = x_train / 255.0\n",
        "x_test_color = x_test / 255.0\n"
      ],
      "metadata": {
        "id": "KjMFZ0wbpPgZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Архитектура автоэнкодера\n",
        "input_img = layers.Input(shape=(32, 32, 1))"
      ],
      "metadata": {
        "id": "lC2Ec2tCpRZm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Энкодер\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)"
      ],
      "metadata": {
        "id": "ZYIIOojTpTeg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Декодер\n",
        "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "yAKjcHhrpVmN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обучение\n",
        "history = autoencoder.fit(\n",
        "    x_train_gray, x_train_color,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_data=(x_test_gray, x_test_color)\n",
        ")\n"
      ],
      "metadata": {
        "id": "BHQ_TpfNpXgw",
        "outputId": "20f08c91-8219-4837-b681-b972caaa1ea4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 26ms/step - loss: 0.0256 - val_loss: 0.0129\n",
            "Epoch 2/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16ms/step - loss: 0.0122 - val_loss: 0.0118\n",
            "Epoch 3/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0115 - val_loss: 0.0112\n",
            "Epoch 4/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0112 - val_loss: 0.0111\n",
            "Epoch 5/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0107 - val_loss: 0.0105\n",
            "Epoch 6/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - loss: 0.0105 - val_loss: 0.0106\n",
            "Epoch 7/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0103 - val_loss: 0.0101\n",
            "Epoch 8/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0101 - val_loss: 0.0102\n",
            "Epoch 9/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0100 - val_loss: 0.0099\n",
            "Epoch 10/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - loss: 0.0098 - val_loss: 0.0106\n",
            "Epoch 11/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0098 - val_loss: 0.0099\n",
            "Epoch 12/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0096 - val_loss: 0.0097\n",
            "Epoch 13/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 14/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0095 - val_loss: 0.0096\n",
            "Epoch 15/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0094 - val_loss: 0.0095\n",
            "Epoch 16/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - loss: 0.0094 - val_loss: 0.0096\n",
            "Epoch 17/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0093 - val_loss: 0.0093\n",
            "Epoch 18/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 0.0092 - val_loss: 0.0094\n",
            "Epoch 19/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - loss: 0.0092 - val_loss: 0.0094\n",
            "Epoch 20/20\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0091 - val_loss: 0.0092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для колоризации\n",
        "def colorize(image):\n",
        "    # Конвертация входного изображения в [0, 1]\n",
        "    image = (image.astype('float32') / 255.0).reshape(1, 32, 32, 1)\n",
        "    prediction = autoencoder.predict(image)\n",
        "    return (prediction[0] * 255).astype('uint8')"
      ],
      "metadata": {
        "id": "0MW7m__FpfEh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка примеров из тестового набора данных\n",
        "samples = [(x_test_gray[i].numpy().squeeze() * 255).astype(np.uint8) for i in range(3)]"
      ],
      "metadata": {
        "id": "bq5ZOKygq_1A"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Интерфейс Gradio\n",
        "interface = gr.Interface(\n",
        "    fn=colorize,\n",
        "    inputs=gr.Image(height=256, width=256, image_mode='L'),  # Увеличиваем размеры ввода\n",
        "    outputs=gr.Image(height=256, width=256),  # Увеличиваем размеры вывода\n",
        "    examples=samples,\n",
        "    title=\"Colorization Autoencoder\",\n",
        "    description=\"Загрузите черно-белое изображение для его колоризации.\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "GiqSskoFpg-w",
        "outputId": "dc4d4e9b-7673-42b9-99a3-fdc03ba887d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a1e09c1d4b5507657c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a1e09c1d4b5507657c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание №2. Реализуйте автоэнкодер для удаления шума на однотипных изображениях\n",
        "\n",
        "1.  Подберите датасет, состоящий из однотипных изображений, которые в реальной жизни часто подвержены зашумлению, например спутниковые снимки, ночные фотографии и т.д.\n",
        "\n",
        "2.  Затем примените к ним операцию зашумления и обучите модель. Также учитывайте тип шума, который Вы применяете. Он должен быть приближен к естественному.\n",
        "\n",
        "Хороший пример реализации подобной задачи: https://www.kaggle.com/code/michalbrezk/denoise-images-using-autoencoders-tf-keras"
      ],
      "metadata": {
        "id": "7V5_gE0uqcmE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pZ0LXQxbqCx2",
        "outputId": "4e22caff-2414-4c93-bd95-953f54680211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.0421 - val_loss: 0.0153\n",
            "Epoch 2/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - loss: 0.0146 - val_loss: 0.0137\n",
            "Epoch 3/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0130 - val_loss: 0.0123\n",
            "Epoch 4/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0120 - val_loss: 0.0117\n",
            "Epoch 5/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.0113 - val_loss: 0.0109\n",
            "Epoch 6/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0108 - val_loss: 0.0105\n",
            "Epoch 7/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0104 - val_loss: 0.0102\n",
            "Epoch 8/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0101 - val_loss: 0.0100\n",
            "Epoch 9/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0098 - val_loss: 0.0098\n",
            "Epoch 10/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0097 - val_loss: 0.0097\n",
            "Epoch 11/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0095 - val_loss: 0.0095\n",
            "Epoch 12/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0094 - val_loss: 0.0094\n",
            "Epoch 13/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0093 - val_loss: 0.0095\n",
            "Epoch 14/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0093 - val_loss: 0.0093\n",
            "Epoch 15/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0092 - val_loss: 0.0092\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b44b3e316efad968b8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b44b3e316efad968b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# Импорт библиотек\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "# Загрузка датасета Fashion MNIST\n",
        "(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "# Нормализация и добавление шума\n",
        "def add_noise(img, noise_factor=0.3):\n",
        "    noisy = img + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=img.shape)\n",
        "    return np.clip(noisy, 0., 1.)\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train_noisy = add_noise(x_train)\n",
        "x_test_noisy = add_noise(x_test)\n",
        "\n",
        "# Добавление канального измерения\n",
        "x_train_noisy = np.expand_dims(x_train_noisy, axis=-1)\n",
        "x_test_noisy = np.expand_dims(x_test_noisy, axis=-1)\n",
        "\n",
        "# Архитектура автоэнкодера\n",
        "input_img = layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Обучение\n",
        "autoencoder.fit(\n",
        "    x_train_noisy, np.expand_dims(x_train, axis=-1),\n",
        "    epochs=15,\n",
        "    batch_size=128,\n",
        "    validation_data=(x_test_noisy, np.expand_dims(x_test, axis=-1))\n",
        ")\n",
        "\n",
        "# Функция для удаления шума\n",
        "def denoise(image):\n",
        "    # Конвертация в градации серого, если изображение цветное\n",
        "    if image.ndim == 3 and image.shape[2] == 3:\n",
        "        image = np.dot(image[..., :3], [0.2989, 0.5870, 0.1140]).astype('float32')\n",
        "    # Добавление канального измерения для черно-белых изображений\n",
        "    if image.ndim == 2:\n",
        "        image = np.expand_dims(image, axis=-1)\n",
        "    # Изменение размера и нормализация\n",
        "    image = tf.image.resize(image, (28, 28)).numpy()\n",
        "    image = image.astype('float32') / 255.0\n",
        "    # Предсказание\n",
        "    prediction = autoencoder.predict(np.expand_dims(image, axis=0))\n",
        "    # Преобразование в формат изображения\n",
        "    output = (prediction[0] * 255).astype('uint8').squeeze()\n",
        "    return output\n",
        "\n",
        "# Подготовка примеров (нормализация к 0-255)\n",
        "samples = [(x_test_noisy[i].squeeze() * 255).astype('uint8') for i in range(3)]\n",
        "\n",
        "# Gradio интерфейс\n",
        "interface = gr.Interface(\n",
        "    fn=denoise,\n",
        "    inputs=gr.Image(height=256, width=256, image_mode='L'),\n",
        "    outputs=gr.Image(height=256, width=256, image_mode='L'),\n",
        "    examples=samples\n",
        ")\n",
        "interface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание №.3 Реализуйте автоэнкодер для улучшения качества изображения путём увеличения его разрешения (апскейлинг-[определение](https://dic.academic.ru/dic.nsf/ruwiki/346555))\n",
        "\n",
        "1. На входной слой нейронной сети подаётся изображение с размерностью (256, 256,3) - X_test. В результате работы нейронной сети на выходном слое должно получиться изображение (512, 512, 3) - Y_test. В итоге мы получаем двукратный апскейл исходного изображения.\n",
        "\n",
        "2. Датасет собираете из изображений, разрешение которых выше эталонного (512,512,3). Затем преобразуете их в указанные размерности и формируете из них обучающую и проверочную выборку.\n",
        "\n",
        "3. Обучите модель. Отобразите графики обучения\n",
        "\n",
        "4. Подумайте, каким образом можно будет адаптировать модель вашей нейронной сети для двухратного апскейла изображения с любой размерностью, большей, чем (256,256,3), с полным или частичным сохранением его исходных пропорций. Например: подаём на вход изображение с разрешением (1920, 1080, 3) - получаем изображение с разрешением (3840, 2160, 3), т.е. с полным сохранением исходных пропорций или (1792, 1024, 3) с частичным сохранением исходных пропорций.\n",
        "\n",
        "> Подсказка: Для этого можно реализовать алгоритм предварительной обработки исходного изображения, перед подачей его в нейронную сеть"
      ],
      "metadata": {
        "id": "-IoRrkhj2R6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт библиотек\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import gradio as gr\n",
        "import cv2\n",
        "\n",
        "# Загрузка и подготовка данных (пример с использованием случайных данных)\n",
        "# В реальности используйте датасет с изображениями >512x512\n",
        "# Например: https://www.tensorflow.org/datasets/catalog/div2k\n",
        "# Для примера создадим искусственные данные\n",
        "def load_data():\n",
        "    (x_train, _), (x_test, _) = tf.keras.datasets.cifar10.load_data()\n",
        "    x_train = tf.image.resize(x_train, (256, 256))  # Имитация исходных данных\n",
        "    x_train_low = tf.image.resize(x_train, (128, 128))\n",
        "    return x_train_low / 255.0, x_train / 255.0\n",
        "\n",
        "x_train_low, x_train_high = load_data()\n",
        "\n",
        "# Архитектура автоэнкодера\n",
        "input_img = layers.Input(shape=(128, 128, 3))\n",
        "\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Обучение\n",
        "autoencoder.fit(\n",
        "    x_train_low, x_train_high,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "# Функция для апскейлинга\n",
        "def upscale(image):\n",
        "    image = tf.image.resize(image, (128, 128))\n",
        "    prediction = autoencoder.predict(np.expand_dims(image, axis=0))\n",
        "    return tf.image.resize(prediction[0], (256, 256))\n",
        "\n",
        "# Gradio интерфейс\n",
        "samples = [x_train_low[i].numpy() for i in range(3)]\n",
        "interface = gr.Interface(\n",
        "    fn=upscale,\n",
        "    inputs=gr.Image(shape=(128, 128)),\n",
        "    outputs=gr.Image(shape=(256, 256)),\n",
        "    examples=samples\n",
        ")\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "vCw97biA2Rlk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}